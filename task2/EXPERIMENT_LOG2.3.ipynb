{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Part III**** - Finally, take the following\n",
    "[dataset](https://www.dropbox.com/s/otc12z2w7f7xm8z/mnistTask3.zip),\n",
    "train on this dataset and provide test accuracy on the MNIST test set,\n",
    "using the same test split from part 2. Train using scratch random\n",
    "initialization and using the pretrained network part 1. Do the same\n",
    "analysis as 2 and report what happens this time. Try and do qualitative\n",
    "analysis of what's different in this dataset. Please save your model\n",
    "checkpoints.\n",
    "\n",
    "****Dataset Overview****\n",
    "\n",
    "On first look the dataset appears to be MNIST itself, containing 60k\n",
    "images and \\[0-9\\] as classes. However, on closer inspection, it is\n",
    "evident that images have not been placed in the correct category, such\n",
    "that every class contains images from every other class except its own\n",
    "i.e.\n",
    "\n",
    "The folder for `0` contains images 1-9, the folder for `1` contains 0,\n",
    "2-9 and so on.\n",
    "\n",
    "Since I have already built a framework for plugging in different\n",
    "datasets, I made use of the same, writing a DataModule\n",
    "(`MNISTWrongModule`) to load the provided dataset. As mentioned, the\n",
    "test dataset is from MNIST and the network was trained from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "from numbers_and_letters import NumbersAndLettersCNN, NumbersAndLettersModule\n",
    "from mnist import MNISTModule\n",
    "from mnist_wrong import MNISTWrongModule\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='numbers_and_letters')\n",
    "parser.add_argument('--pretrained', default=False)\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.dataset = 'mnist_wrong'\n",
    "\n",
    "SAVE_PATH = \"models/\"\n",
    "MODEL_NAME = '5conv1fc_mnist_wrong'\n",
    "LOAD_MODEL_NAME = '5conv1fc_numbers'\n",
    "BATCH_SIZE = 32\n",
    "NUMBERS_ONLY = True\n",
    "\n",
    "if args.dataset == 'numbers_and_letters':\n",
    "    BASE_DIR = \"train\"\n",
    "    INPUT_DIM = torch.tensor([3, 900, 1200])\n",
    "\n",
    "    # Create DataModule to handle loading of dataset\n",
    "    data_module = NumbersAndLettersModule(BASE_DIR, BATCH_SIZE, NUMBERS_ONLY)\n",
    "    model = NumbersAndLettersCNN(INPUT_DIM, len(data_module.img_labels),\n",
    "                                 data_module.img_labels, NUMBERS_ONLY)\n",
    "elif args.dataset == 'mnist':\n",
    "    data_module = MNISTModule(BATCH_SIZE)\n",
    "    INPUT_DIM = torch.tensor([1, 28, 28])\n",
    "    model = NumbersAndLettersCNN(INPUT_DIM, 10, ['0','1','2','3','4',\n",
    "                                                 '5','6','7','8','9'], NUMBERS_ONLY)\n",
    "elif args.dataset == 'mnist_wrong':\n",
    "    data_module = MNISTWrongModule(BATCH_SIZE)\n",
    "    INPUT_DIM = torch.tensor([1, 28, 28])\n",
    "    model = NumbersAndLettersCNN(INPUT_DIM, 10, ['0','1','2','3','4',\n",
    "                                                 '5','6','7','8','9'], NUMBERS_ONLY)\n",
    "else:\n",
    "    print(\"Invalid dataset choice\")\n",
    "    exit(0)\n",
    "\n",
    "# Log metrics to WandB\n",
    "wandb_logger = pl.loggers.WandbLogger(save_dir='logs/',\n",
    "                                        name=MODEL_NAME,\n",
    "                                        project='midas-task-2')\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=1\n",
    ")\n",
    "\n",
    "if args.pretrained:\n",
    "    model.load_state_dict(torch.load(os.path.join(SAVE_PATH, LOAD_MODEL_NAME),\n",
    "                                     map_location=torch.device('cuda')))\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, logger=wandb_logger,\n",
    "                     callbacks=[early_stopping])\n",
    "trainer.fit(model, data_module)\n",
    "trainer.test(model=model, datamodule=data_module)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_PATH, MODEL_NAME))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Summary****\n",
    "\n",
    "|                         |            |\n",
    "|-------------------------|------------|\n",
    "| **Metric**              | **Result** |\n",
    "| Test Accuracy (%)       | 0.20       |\n",
    "| Validation Accuracy (%) | 11.55      |\n",
    "| Epochs trained          | 4          |\n",
    "| Training time (min)     | 6m 16s     |\n",
    "\n",
    "As expected, the model has learnt incredibly poorly. Every class\n",
    "contains images from 9 other classes, making learning a model and\n",
    "classifying an impossible task. Even with a powerful model like a CNN,\n",
    "without quality data the model will not be able to learn any meaningful\n",
    "representation and will perform poorly and this task illustrates that.\n",
    "\n",
    "The validation accuracy hovers around 10%, i.e. the expected performance\n",
    "when choosing randomly among 10 classes, indicating that the network\n",
    "cannot really distinguish between them.\n",
    "\n",
    "Finally, the training is repeated, but this time, a pretrained model is\n",
    "used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = '5conv1fc_mnist_wrong_pretrained'\n",
    "LOAD_MODEL_NAME = '5conv1fc_numbers'\n",
    "data_module = MNISTWrongModule(BATCH_SIZE)\n",
    "INPUT_DIM = torch.tensor([1, 28, 28])\n",
    "model = NumbersAndLettersCNN(INPUT_DIM, 10, ['0','1','2','3','4',\n",
    "                                                '5','6','7','8','9'], NUMBERS_ONLY)\n",
    "# Log metrics to WandB\n",
    "wandb_logger = pl.loggers.WandbLogger(save_dir='logs/',\n",
    "                                        name=MODEL_NAME,\n",
    "                                        project='midas-task-2')\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    ")\n",
    "\n",
    "# Load pretrained model\n",
    "model.load_state_dict(torch.load(os.path.join(SAVE_PATH, LOAD_MODEL_NAME),\n",
    "                                     map_location=torch.device('cuda')))\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, logger=wandb_logger,\n",
    "                     callbacks=[early_stopping])\n",
    "trainer.fit(model, data_module)\n",
    "trainer.test(model=model, datamodule=data_module)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_PATH, MODEL_NAME))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Summary****\n",
    "\n",
    "|                         |            |\n",
    "|-------------------------|------------|\n",
    "| **Metric**              | **Result** |\n",
    "| Test Accuracy (%)       | 0.19       |\n",
    "| Validation Accuracy (%) | 11.03      |\n",
    "| Epochs trained          | 4          |\n",
    "| Training time (min)     | 6m 57s     |\n",
    "\n",
    "Pretraining the model did not have any significant effect on the\n",
    "accuracy of the model and it still hovers below 1%. As before, the\n",
    "validation accuracy hovers around 10%, indicating that the model isn't\n",
    "learning well.\n",
    "\n",
    "In summary, because there is such a large amount of wrongly labeled\n",
    "data, the effects of the pretraining are wiped away quickly and the\n",
    "accuracy descends to initial levels."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
