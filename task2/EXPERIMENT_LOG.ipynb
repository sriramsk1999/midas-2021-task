{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365691d6",
   "metadata": {},
   "source": [
    "****Part I**** - Use this\n",
    "[dataset](https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip) to\n",
    "train a CNN. Use no other data source or pretrained networks, and\n",
    "explain your design choices during preprocessing, model building and\n",
    "training. Also, cite the sources you used to borrow techniques. A test\n",
    "set will be provided later to judge the performance of your classifier.\n",
    "Please save your model checkpoints.\n",
    "\n",
    "****Dataset Overview****\n",
    "\n",
    "The dataset consists of 62 classes, with 40 samples of each sample.\n",
    "Classes - \\[0-9, A-Z, a-z\\] Total number of images - 62\\*40 = 2480\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72242c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image details\n",
    "import torch\n",
    "from skimage import io\n",
    "\n",
    "img = io.imread(\"train/Sample001/img001-001.png\")\n",
    "io.imshow(img)\n",
    "print(torch.tensor(img).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3c546",
   "metadata": {},
   "source": [
    "****Establishing baselines****\n",
    "\n",
    "In solving this task, my first approach was to establish a baseline\n",
    "using a barebones CNN with minimal features. The outcome of this subtask\n",
    "would be to assess performance on the dataset and provide a baseline for\n",
    "future measurements. Pytorch and the Pytorch-lightning framework are\n",
    "used in developing the neural network for this task. Pytorch Lightning\n",
    "provides a high level API in developing the networks, while organizing\n",
    "the code and making it easier to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c67f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from skimage import io\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12688882",
   "metadata": {},
   "source": [
    "****Data Loading****\n",
    "\n",
    "For handling all tasks relating to data and its loading, Pytorch\n",
    "Lightning provides `LightningDataModule` [1], which wraps Pytorch's\n",
    "`Dataset` and `DataLoader` abstractions, and provides a single interface\n",
    "to access data.\n",
    "\n",
    "The image paths and the classes to which they belong to are loaded from\n",
    "the provided base directory. Initially, the images themselves were\n",
    "loaded into memory instead of their filepaths, but my system was unable\n",
    "to load the entrie dataset due to lack of memory. Thus the paths are\n",
    "loaded, and during training, the images will be loaded using these\n",
    "paths.\n",
    "\n",
    "Cross-entropy is widely used as a loss function when optimizing\n",
    "classification models [2], allowing us to quantify the difference\n",
    "between the target probability distribution and the distribution of the\n",
    "model's output.\n",
    "\n",
    "In PyTorch, CrossEntropyLoss [3] expects *a list of class indices \\[0,\n",
    "C-1\\]*, which means the target vector of classnames must be encoded into\n",
    "a vector of integers. sklearn's `LabelEncoder()` transforms the target\n",
    "vector into the appropriate encoding.\n",
    "\n",
    "Finally, `train_test_split` is used to split the dataset into training,\n",
    "validation and testing sets according to an 80-10-10 split.\n",
    "\n",
    "[1] [LightningDataModule](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html)\n",
    "\n",
    "[2] [A Gentle Introduction to Cross-Entropy for Machine\n",
    "Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)\n",
    "\n",
    "[3] [PyTorch\n",
    "CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumbersAndLettersDataset(Dataset):\n",
    "    ''' Dataset for numbers and letters. '''\n",
    "    def __init__(self, input_data, target, transform=None):\n",
    "        self.input_data = input_data\n",
    "        self.target = target\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.tensor(io.imread(self.input_data[idx]))\n",
    "        img = img.permute(2, 0, 1) # Reshape to bring channels to first index\n",
    "        return (img, self.target[idx])\n",
    "\n",
    "class NumbersAndLettersModule(pl.LightningDataModule):\n",
    "    ''' DataModule for loading of dataset. '''\n",
    "    def __init__(self, data_dir, batch_size):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.nal_train = None\n",
    "        self.nal_test = None\n",
    "        self.nal_val = None\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage in (None, 'fit'): # Create all datasets\n",
    "            img_dataset, img_classes = self.load_data(self.data_dir)\n",
    "            print(\"Data loaded from disk\")\n",
    "\n",
    "            # Prepare target using Label Encoding\n",
    "            le = LabelEncoder()\n",
    "            le.fit(img_classes)\n",
    "            img_classes = torch.tensor(le.transform(img_classes))\n",
    "\n",
    "            dataset = NumbersAndLettersDataset(img_dataset, img_classes)\n",
    "\n",
    "            # Creating train, test, val datasets according to an 80-10-10 split\n",
    "            self.nal_train, self.nal_test = train_test_split(dataset, test_size=0.1)\n",
    "            self.nal_train, self.nal_val = train_test_split(self.nal_train, test_size=0.1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.nal_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.nal_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.nal_test, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def load_data(self, img_dir):\n",
    "        ''' Load image_paths and their classes from disk. '''\n",
    "        dataset = []\n",
    "        classes = []\n",
    "        for folder in os.listdir(img_dir):\n",
    "            img_class = int(folder[-2:]) # Extract last 2 digits of folder name\n",
    "            if img_class < 11:\n",
    "                img_class = str(img_class - 1) # 0-9\n",
    "            elif img_class < 37:\n",
    "                img_class = chr(img_class + 54) # A-Z\n",
    "            else: img_class = chr(img_class + 60) # a-z\n",
    "            for img in os.listdir(os.path.join(img_dir, folder)):\n",
    "                img_path = os.path.join(img_dir, folder, img)\n",
    "                dataset.append(img_path)\n",
    "                classes.append(img_class)\n",
    "        return dataset, classes\n",
    "\n",
    "SEED = 42 # Set a global seed for reproducible results\n",
    "BATCH_SIZE = 4\n",
    "BASE_DIR = \"train\"\n",
    "\n",
    "INPUT_DIM = torch.tensor([3, 900, 1200])\n",
    "\n",
    "pl.utilities.seed.seed_everything(SEED)\n",
    "\n",
    "# Create DataModule to handle loading of dataset\n",
    "data_module = NumbersAndLettersModule(BASE_DIR, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71c068",
   "metadata": {},
   "source": [
    "In building the CNN, I decided to implement a simple CNN to understand\n",
    "what the baseline performance looks like. Thus, the design decision in\n",
    "building this CNN were guided by using standard domain knowledge without\n",
    "additional features.\n",
    "\n",
    "**Network Architecture**\n",
    "\n",
    "In designing the network, I referred to CS231, Stanford's course on CNNs\n",
    "[1], in which they present a stacked CONV-RELU followed by POOL layers\n",
    "as the most common architecture. I have opted for a similar design of 4\n",
    "repeating CONV-POOL-RELU stacks. After this, the output of the 4th stack\n",
    "is flattened into a 1D vector and then fed to three fully connected\n",
    "layers, gradually bringing down the dimensionality of the vector to that\n",
    "of the number of output classes.\n",
    "\n",
    "In PytorchLightning, the `LightningModule` is used to build neural nets,\n",
    "and it exposes various methods to simplify the process.\n",
    "\n",
    "1.  `forward` -\\> The forward pass.\n",
    "2.  `configure_optimizers` -\\> Return the optimizer to be used in\n",
    "    training\n",
    "3.  `{training, validation, test}_step` -\\> These functions expose the\n",
    "    training, validation and test loops respectively. In these\n",
    "    functions, the input is propagated through the network, following\n",
    "    which the cross entropy loss is computed.\n",
    "\n",
    "*Loss* -\\> As common in classification tasks, Cross Entropy Loss is\n",
    "used. *Optimizer* -\\> Again, the standard choice is the Adam optimizer\n",
    "and it has been selected.\n",
    "\n",
    "**Logging**\n",
    "\n",
    "I consider logging very important, especially while experimenting with\n",
    "different architectures, it is important to have a baseline stored so\n",
    "future changes can be compared against it. For logging, I have used the\n",
    "Weights and Biases logger integrated into Pytorch Lightning. It allows\n",
    "for live tracking on their website and export of results.\n",
    "\n",
    "[1] [CS231N](https://cs231n.github.io/convolutional-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b03569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NumbersAndLettersCNN(pl.LightningModule):\n",
    "    ''' Implementation of CNN to detect numbers and letters. '''\n",
    "    def __init__(self, input_dim, output_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_dim[0], 16, 6)\n",
    "        self.conv2 = nn.Conv2d(16, 64, 6)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 6)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 6)\n",
    "        self.pool = nn.MaxPool2d(3)\n",
    "        self.fc1 = nn.Linear(256 * 8 * 12, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 256)\n",
    "        self.fc3 = nn.Linear(256, output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Forward pass '''\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x.float())\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x.float())\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        acc = torch.mean((torch.argmax(output, axis=1) == y).float())\n",
    "        self.log_dict({'val_loss': loss, 'val_acc': acc})\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x.float())\n",
    "        loss = F.cross_entropy(output, y.long())\n",
    "        acc = torch.mean((torch.argmax(output, axis=1) == y).float())\n",
    "        self.log_dict({'test_loss': loss, 'test_acc': acc})\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "MODEL_NAME = 'numbers-and-letters-cnn'\n",
    "\n",
    "INPUT_DIM = torch.tensor([3, 900, 1200])\n",
    "OUTPUT_CLASSES = 62\n",
    "\n",
    "# Train and test model\n",
    "\n",
    "model = NumbersAndLettersCNN(INPUT_DIM, OUTPUT_CLASSES)\n",
    "\n",
    "# Log metrics to WandB\n",
    "wandb_logger = pl.loggers.WandbLogger(save_dir='logs/',\n",
    "                                        name=\"%s.pth\" %MODEL_NAME,\n",
    "                                        project='midas-task-2')\n",
    "trainer = pl.Trainer(gpus=1, logger=wandb_logger, max_epochs=1)\n",
    "trainer.fit(model, data_module)\n",
    "trainer.test(model=model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c347f82",
   "metadata": {},
   "source": [
    "memory issues -\\> below ran out of memory with batch size 64 increase\n",
    "filter size and maxpool size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
